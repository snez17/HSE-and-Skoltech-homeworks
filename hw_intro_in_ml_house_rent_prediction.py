# -*- coding: utf-8 -*-
"""HW_Intro in ML_house-rent-prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1muKbfDBUXT0ZTKpOfA8uFHEHW_Cq3Ypi
"""

!pip install opendatasets
!pip install pandas

import opendatasets as od
import pandas as pd

od.download(
    "https://www.kaggle.com/datasets/iamsouravbanerjee/house-rent-prediction-dataset/data")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from wordcloud import WordCloud
from sklearn import metrics
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

import warnings
warnings.filterwarnings('ignore')

# reading the csv file
file =('house-rent-prediction-dataset/\
House_Rent_Dataset.csv')
rent_data = pd.read_csv(file)

# displaying the contents of the csv file
rent_data.head(2)

##удалим неинформативные столбцы из датасета
rent_data = rent_data.drop(['Posted On','Area Locality','Floor'],axis=1)
rent_data.head()

#введем фиктивные переменные, тем самым переведем категориальные переменные в числовые
rent_data = pd.get_dummies(rent_data, columns=['Area Type', 'City', 'Furnishing Status', 'Tenant Preferred', 'Point of Contact'])
rent_data.head()

X = rent_data.drop('Rent',axis=1)
y = rent_data['Rent']

X.head(1)

"""**1. Получить решение задачи в замкнутом виде с помощью псевдообратной
матрицы. Рассмотреть случаи без регуляризации и с L регуляризацией**

1.1. без регуляризации
"""

A = np.linalg.pinv(X)
W = np.dot(A,y)
W

len(X.columns)

len(W)

X.columns

Y_pred_1 = W[0]*X['BHK']+ W[1]*X['Size']+W[2]*X['Bathroom']+ W[3]*X['Area Type_Built Area'] + W[4]*X['Area Type_Carpet Area'] +W[5]*X['Area Type_Super Area']+W[6]*X['City_Bangalore'] +W[7]*X['City_Chennai'] + W[8]*X['City_Delhi'] +W[9]*X['City_Hyderabad'] +W[10]*X['City_Kolkata'] +W[11]*X['City_Mumbai'] + W[12]*X['Furnishing Status_Furnished'] +W[13]*X['Furnishing Status_Semi-Furnished'] +W[14]*X['Furnishing Status_Unfurnished']+ W[15]*X['Tenant Preferred_Bachelors'] +W[16]*X['Tenant Preferred_Bachelors/Family'] + W[17]*X['Tenant Preferred_Family'] + W[18]*X['Point of Contact_Contact Agent'] + W[19]*X['Point of Contact_Contact Builder'] +W[20]*X['Point of Contact_Contact Owner']
Y_pred_1

loss_1 = mean_squared_error(y, Y_pred_1, squared = False)
loss_1

"""1.2. с регуляризацией"""

class RidgeReg():

  def __init__(self, alpha = 1.0):

    self.alpha = alpha
    self.thetas = None

  def fit(self, x, y):

    x = x.copy()
    x = self.add_ones(x)
    I = np.identity(x.shape[1])
    I[0][0] = 0

    self.thetas = np.linalg.inv(x.T.dot(x) + self.alpha * I).dot(x.T).dot(y)

  def predict(self, x):

    x = x.copy()
    x = self.add_ones(x)
    return np.dot(x, self.thetas)

  def add_ones(self, x):
    return np.c_[np.ones((len(x), 1)), x]
ridge = RidgeReg(alpha = 10)

ridge.fit(X_train, y_train)

y_pred_train = ridge.predict(X_train)
y_pred_test = ridge.predict(X_test)

print('train: ' + str(mean_squared_error(y_train, y_pred_train, squared = False)))
print('test: ' + str(mean_squared_error(y_test, y_pred_test, squared = False)))

"""**2. Получить решение задачи методом градиентного спуска. Получить формулу
для градиента, используя матрицу признаков. Сравнить полученные решение с
п.1.**
"""

class LinearModel:
    """
    Linear Regression Model Class
    """
    def __init__(self):
        pass

    def gradient_descent(self, X, y, learn_rate=0.01, n_iters=100):
        """
        Trains a linear regression model using gradient descent
        """
        n_samples, n_features = X.shape
        self.weights = np.zeros(shape=(n_features,1))
        self.bias = 0
        self.prev_weights = []
        self.prev_bias = []
        self.X = X
        self.y = y
        costs = []

        for i in range(n_iters):
            """"
            Training Phase
            """
            y_hat = np.dot(X, self.weights) + self.bias
            """
            Cost error Phase
            """
            cost = (1 / n_samples) * np.sum((y_hat - y)**2)
            costs.append(cost)
            """
            Verbose: Description of cost at each iteration
            """
            if i % 200 == 0:
                print("Cost at iteration {0}: {1}".format(i,cost))
            """
            Updating the derivative
            """
            Delta_w = (2 / n_samples) * np.dot(X.T, (y_hat - y))
            Delta_b = (2 / n_samples) * np.sum((y_hat - y))

            """"
            Updating weights and bias
            """
            self.weights = self.weights - learn_rate * Delta_w
            self.bias = self.bias - learn_rate * Delta_b

            """
            Save the weights for visualisation
            """
            self.prev_weights.append(self.weights)
            self.prev_bias.append(self.bias)

        return self.weights, self.bias, costs

    def predict(self, X):
        """
        Predicting the values by using Linear Model
        """
        return np.dot(X, self.weights) + self.bias

y_train= y_train.reshape(-1,1)
y_test= y_test.reshape(-1,1)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
y_train = sc_X.fit_transform(y_train)
y_test = sc_y.fit_transform(y_test)

model = LinearModel()

w_trained, b_trained, costs = model.gradient_descent(X_train, y_train, learn_rate=0.005, n_iters=500)

w_trained

Y_pred_2 = w_trained[0]*X['BHK']+ w_trained[1]*X['Size']+w_trained[2]*X['Bathroom']+ w_trained[3]*X['Area Type_Built Area'] + w_trained[4]*X['Area Type_Carpet Area'] +w_trained[5]*X['Area Type_Super Area']+w_trained[6]*X['City_Bangalore'] +w_trained[7]*X['City_Chennai'] + w_trained[8]*X['City_Delhi'] +w_trained[9]*X['City_Hyderabad'] +w_trained[10]*X['City_Kolkata'] +w_trained[11]*X['City_Mumbai'] + w_trained[12]*X['Furnishing Status_Furnished'] +w_trained[13]*X['Furnishing Status_Semi-Furnished'] +w_trained[14]*X['Furnishing Status_Unfurnished']+ w_trained[15]*X['Tenant Preferred_Bachelors'] +w_trained[16]*X['Tenant Preferred_Bachelors/Family'] + w_trained[17]*X['Tenant Preferred_Family'] + w_trained[18]*X['Point of Contact_Contact Agent'] + w_trained[19]*X['Point of Contact_Contact Builder'] +w_trained[20]*X['Point of Contact_Contact Owner']
Y_pred_2

loss_1 = mean_squared_error(y, Y_pred_2, squared = False)
loss_1

"""**1. Какие признаки оказывают наибольший вклад в точность определения
стоимости аренды? Предложить способы отбора наиболее важных признаков**

Для оценки вклада (значимости) каждого признака в точность определения стоимости аренды можно использовать тест на p-value.

Также можно использовать методы LIME, SHAP

**2. Какая модель имеет наименьшее значение функции потерь на тестовой
выборке? Помогает ли регуляризация избежать эффекта переобучения в
данном примере?**

Наименьшее значение функции - в модели с применением псевдообратной матрицы. Регуляризация помогает избежать переобучения в данном примере.

При решении задачи использовались материалы:

1. https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html

2. https://www.kaggle.com/code/prashantverma13/house-rent-prediction-in-depth-analysis-models

3. https://vxy10.github.io/2016/06/25/lin-reg-matrix/

4. https://dev.to/ml_scratch/mls-1-b-gradient-descent-in-linear-regression-53dl

5. https://www.geeksforgeeks.org/ordinary-least-squares-and-ridge-regression-variance-in-scikit-learn/
"""