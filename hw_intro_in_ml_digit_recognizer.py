# -*- coding: utf-8 -*-
"""HW_Intro in ML_digit-recognizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O85y3lxQNvvUitssdsVHSPKPLb_Coyeh
"""

!pip install opendatasets
!pip install pandas

import opendatasets as od
import pandas as pd

od.download(
    "https://www.kaggle.com/competitions/digit-recognizer/data")

# reading the csv file
file =('digit-recognizer/\
train.csv')
train = pd.read_csv(file)

# displaying the contents of the csv file
train.head(2)

# reading the csv file
file =('digit-recognizer/\
test.csv')
test = pd.read_csv(file)

# displaying the contents of the csv file
test.head(2)

pip install np_utils

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
# %matplotlib inline

np.random.seed(2)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

import keras
from keras.utils import to_categorical #convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

from keras.layers import Conv2D, Lambda, MaxPooling2D # convolution layers
from keras.layers import Dense, Dropout, Flatten # core layers

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import BatchNormalization

sns.set(style='white', context='notebook', palette='deep')

"""Для представленного датасета необходимо:

**1. Решить задачу многоклассовой классификации с помощью многослойной
нейронной сети, состоящей из полносвязных слоев.**
"""

#выводим зависимую переменную (значение цифры) в отдельный датасет
Y_train = train["label"]

#создаем новый датасет на основе изначального и удаляем зависимую переменную из датасета
X_train = train.drop(labels = ["label"],axis = 1)

#смотрим, как часто попадаются цифры
Y_train.value_counts()

#визуализируем
Y_train = pd.DataFrame(Y_train)
g = sns.countplot(x='label', data=Y_train)

# Check the data
X_train.isnull().any().describe()

test.isnull().any().describe()

# поскольку максимальное значение - 255, мы поделим на него и получим значения в диапазоне от 0 до 1
X_train = X_train / 255.0
test = test / 255.0

type(X_train)

# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)
X_train = X_train.reshape(-1,28,28,1)
test = test.reshape(-1,28,28,1)

# применим One hot encoding – метод представления категориальных данных в виде числовых векторов (например : 2 -> [0,0,1,0,0,0,0,0,0,0])
Y_train = to_categorical(Y_train, num_classes = 10)

# Set the random seed
random_seed = 2

# Split the train and the validation set for the fitting
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)

# пример. Функция imshow принимает входное изображение в виде двумерного или трехмерного массива и отображает его на координатной плоскости
g = plt.imshow(X_train[0][:,:,0])

"""Полносвязный слой - слой, в котором каждый нейрон соединен со всеми нейронами на предыдущем уровне, причем каждая связь имеет свой весовой коэффициент.

1 модель
"""

model=Sequential()

#model.add(Lambda(standardize,input_shape=(28,28,1)))
model.add(Conv2D(filters=64, kernel_size = (3,3), activation="relu", input_shape=(28,28,1)))
model.add(Conv2D(filters=64, kernel_size = (3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())

model.add(Conv2D(filters=128, kernel_size = (3,3), activation="relu"))
model.add(Conv2D(filters=128, kernel_size = (3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())

model.add(Conv2D(filters=256, kernel_size = (3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(512,activation="relu"))

model.add(Dense(10,activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

model.summary()

# With data augmentation to prevent overfitting

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.01, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images


#datagen.fit(X_train)
train_gen = datagen.flow(X_train, Y_train, batch_size=128)
test_gen = datagen.flow(X_val, Y_val, batch_size=128)

epochs = 3
batch_size = 128
train_steps = X_train.shape[0] // batch_size
valid_steps = X_val.shape[0] // batch_size

es = keras.callbacks.EarlyStopping(
        monitor="val_acc", # metrics to monitor
        patience=10, # how many epochs before stop
        verbose=1,
        mode="max", # we need the maximum accuracy.
        restore_best_weights=True, #
     )

rp = keras.callbacks.ReduceLROnPlateau(
        monitor="val_acc",
        factor=0.2,
        patience=3,
        verbose=1,
        mode="max",
        min_lr=0.00001,
     )

# Fit the model
history = model.fit_generator(train_gen,
                              epochs = epochs,
                              steps_per_epoch = train_steps,
                              validation_data = test_gen,
                              validation_steps = valid_steps,
                              callbacks=[es, rp])

from  tensorflow.keras.utils import plot_model
plot_model(model, to_file='CNN_model_arch.png', show_shapes=True, show_layer_names=True)

# Plot the loss and accuracy curves for training and validation
fig, ax = plt.subplots(2,1, figsize=(18, 10))
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True)

fig = plt.figure(figsize=(10, 10)) # Set Figure

y_pred = model.predict(X_val) # Predict class probabilities as 2 => [0.1, 0, 0.9, 0, 0, 0, 0, 0, 0, 0]

Y_pred = np.argmax(y_pred, 1) # Decode Predicted labels
Y_test = np.argmax(Y_val, 1) # Decode labels

mat = confusion_matrix(Y_test, Y_pred) # Confusion matrix

# Plot Confusion matrix
sns.heatmap(mat.T, square=True, annot=True, cbar=False, cmap=plt.cm.Blues, fmt='.0f')
plt.xlabel('Predicted Values')
plt.ylabel('True Values');
plt.show();

y_pred = model.predict(X_val)
X_val__ = X_val.reshape(X_val.shape[0], 28, 28)

fig, axis = plt.subplots(4, 4, figsize=(12, 14))
for i, ax in enumerate(axis.flat):
    ax.imshow(X_val__[i], cmap='binary')
    ax.set(title = f"Real Number is {Y_val[i].argmax()}\nPredict Number is {y_pred[i].argmax()}");

"""2 модель"""

# Set the CNN model
# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out

model = Sequential()

model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))


model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',
                 activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))


model.add(Flatten())
model.add(Dense(256, activation = "relu"))
model.add(Dropout(0.5))
model.add(Dense(10, activation = "softmax"))

"""Процесс обучения модели глубокого обучения включает в себя минимизацию функции потерь, которая измеряет разницу между прогнозируемым результатом и фактическим результатом. Это делается путем настройки весов и смещений нейронной сети с помощью процесса, называемого обратным распространением, который использует алгоритмы оптимизации для обновления параметров.

Стохастический градиентный спуск (SGD) — широко используемый алгоритм оптимизации в машинном и глубоком обучении. Это вариант градиентного спуска, при котором параметры модели обновляются с использованием небольшого подмножества обучающих данных, называемого мини-батчем, а не всего набора данных.

RMSprop — это алгоритм оптимизации, который решает проблему медленной сходимости в стохастическом градиентном спуске (SGD) за счет адаптивного масштабирования скорости обучения для каждого параметра. Алгоритм основан на концепции экспоненциальных скользящих средних квадратов градиентов.
"""

# оптимизируем подбор параметров для модели
optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08)

# передаем оптимизатор в модель в качестве аргументов метода compile()
model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])

outputTensor = model.output
print(outputTensor)

listOfVariableTensors = model.trainable_weights
print(listOfVariableTensors)

from tensorflow.keras.applications import VGG16
from tensorflow.keras import backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution()


model = VGG16(weights='imagenet',
              include_top=False)

layer_name = 'block3_conv1'
filter_index = 0

layer_output = model.get_layer(layer_name).output
loss = K.mean(layer_output[:, :, :, filter_index])

grads = K.gradients(loss, model.input)[0]

print(model.trainable_weights[0])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
                                            patience=3,
                                            verbose=1,
                                            factor=0.5,
                                            min_lr=0.00001)

epochs = 3 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86

# With data augmentation to prevent overfitting (accuracy 0.99286)
#ImageDataGenerator - класс, генерирующий изображения

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(X_train)

# Fit the model
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction])

"""**2. Вычислить градиент целевой функции по обучаемым параметрам нейронной
сети, сравнить полученные значения со значениями, вычисленными с помощью
библиотечных функций**

**3. Определить параметры нейронной сети, при которых точность классификации
максимальна**
"""

for layer in model.layers: print(layer.get_config(), layer.get_weights())

# Confusion Matrix (Матрица ошибок)  - матрица сопоставления ответов алгоритма и истинных меток объекта

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Predict the values from the validation dataset
Y_pred = model.predict(X_val)
# Convert predictions classes to one hot vectors
Y_pred_classes = np.argmax(Y_pred,axis = 1)
# Convert validation observations to one hot vectors
Y_true = np.argmax(Y_val,axis = 1)
# compute the confusion matrix
confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)
# plot the confusion matrix
plot_confusion_matrix(confusion_mtx, classes = range(10))

# predict results
results = model.predict(test)

# select the indix with the maximum probability
results = np.argmax(results,axis = 1)

results = pd.Series(results,name="Label")

submission = pd.concat([pd.Series(range(1,28001),name = "ImageId"),results],axis = 1)

submission.to_csv("cnn_mnist_datagen.csv",index=False)

"""При выполнении задачи использовались:

1. https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6

2. https://skine.ru/articles/708895/?ysclid=lqia0s7keu561657315

3. https://habr.com/ru/companies/otus/articles/714754/

4. http://www.100byte.ru/python/factors/factors.html?ysclid=lqibuouiat248825058#p2_2

5. https://practicum.yandex.ru/blog/svertochnye-neyronnye-seti/#parametry

6. https://neurohive.io/ru/osnovy-data-science/osnovy-nejronnyh-setej-algoritmy-obuchenie-funkcii-aktivacii-i-poteri/

7. https://habr.com/ru/companies/skillfactory/articles/565232/
"""