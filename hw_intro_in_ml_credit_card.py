# -*- coding: utf-8 -*-
"""HW_Intro in ML_credit_card

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n-p4sqaPkGDf3AWBPmZnUfH7wCPBn356

Метод k-means - aлгоритм EM (expectation-maximization) для кластерного анализа объектов. Основная идея алгоритма заключается в минимизации суммарного квадратичного отклонения точек кластеров от
центров этих кластеров.

Процедура кластерного анализа:

1. Заранее определяется число кластеров k.
2. Для анализа выбирается k точек — центры кластеров.
3. Объект приписывается к тому кластеру, чей центр ближайший.
4. Центр кластера — центр тяжести объектов кластера.
5. Повторяем поочерёдно пункты 3 и 4 до тех пор, пока центры кластеров не перестанут двигаться.

Методы инициализации (установление начальных центроидов):

• Метод Forgy. Из имеющегося набора данных случайным образом
выбираются $ наблюдений.

• Случайное разбиение. Каждому из наблюдений на начальном
этапе случайным образом присваивается номер кластера.

• Алгоритм k-means++. Из имеющегося набора данных случайным
образом выбирается одна точка (первый центроид). Затем
следующая точка выбирается из оставшихся с вероятностью,
пропорционально зависящей от квадрата расстояния от точки до
ближайшего центроида.

Ограничения k-means:

1. Необходимо заранее определить число кластеров
2. Используется только евклидово растояние
3. Результат зависит от начальных центров кластеров
"""

!pip install opendatasets
!pip install pandas

import opendatasets as od
import pandas as pd

od.download(
    "https://www.kaggle.com/datasets/arjunbhasin2013/ccdata")

# reading the csv file
file =('ccdata/\
CC GENERAL.csv')
data = pd.read_csv(file)

# displaying the contents of the csv file
data.head(2)

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

import warnings
warnings.filterwarnings(action="ignore")

"""Задание 3. Для представленного датасета необходимо решить задачу кластеризации методом k-средних. При решении задачи необходимо ответить на следующие вопросы:
1. Как выбор начальных центров кластеров влияет на результат кластеризации
2. Какую метрику выбрать для оценки качества кластеризации?
"""

data.info()

#посмотрим на тип данных

#удалим ID, так как он неинформативен
data.drop(['CUST_ID'], axis=1, inplace=True)

data.corr()

#видим, что есть коррелирующие (>0.4) переменные, можно будет уменьшить размерность через PCA

data.describe()

#видим, что значение в 75% и max - разительно отличаются. значит - есть выбросы.
#пренебрежем ими

"""Делаем предобработку данных (вычищаем выбросы)"""

#посмотрим на кол-во нулей в столбцах
data.isnull().sum().sort_values(ascending=False).head()

#заменим в выявленных столбцах пропуски на средние значения
data.loc[(data['MINIMUM_PAYMENTS'].isnull()==True),'MINIMUM_PAYMENTS']=data['MINIMUM_PAYMENTS'].mean()
data.loc[(data['CREDIT_LIMIT'].isnull()==True),'CREDIT_LIMIT']=data['CREDIT_LIMIT'].mean()

# проверим, что нулей не осталось
data.isnull().sum().sort_values(ascending=False).head()

cols = ['BALANCE', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES',
        'CASH_ADVANCE', 'ONEOFF_PURCHASES_FREQUENCY','PURCHASES_INSTALLMENTS_FREQUENCY',
        'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT',
        'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT']

for col in cols:
    data[col] = np.log(1 + data[col])

"""Примением метод главных компонент, чтобы сократить размерность данных (как мы заметили, есть корреляции)"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  #PCA должны объяснять 95% разброса
X_red = pca.fit_transform(data)

"""Метод локтя является популярным методом определения оптимального количества кластеров. Здесь мы вычисляем сумму квадратов ошибок внутри кластера (WCSS) для различных значений k и выбираем k, для которого WSS первым начинает уменьшаться. На графике WSS-versus-k это можно наблюдать в виде изгиба."""

from sklearn.cluster import KMeans

kmeans_models = [KMeans(n_clusters=k, random_state=23).fit(X_red) for k in range (1, 10)]
innertia = [model.inertia_ for model in kmeans_models]

plt.plot(range(1, 10), innertia)
plt.title('Elbow method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

"""Давайте теперь построим график зависимости WCSS от K кластеров. Ниже видно, что при K=3 наблюдается сгиб локтя, т.е. это точка, после которой WCSS не сильно уменьшается с увеличением значения K.

**Какую метрику выбрать для оценки качества кластеризации?**

Выделяют внешние и внутренние метрики качества. Внешние используют информацию об истинном разбиении на кластеры, в то время как внутренние метрики не используют никакой внешней информации и оценивают качество кластеризации, основываясь только на наборе данных.

Поскольку нам неизвестны истинные значения кластеров (как и их количество), то будем использовать наиболее распространенные из внутренних метрик качества:
- Силуэт (Silhouette)
- Индекс Calinski–Harabasz
- Индекс Дэвиcа-Болдуина (Davies–Bouldin Index)
"""

from sklearn.metrics import silhouette_score

silhoutte_scores = [silhouette_score(X_red, model.labels_) for model in kmeans_models[1:6]]
plt.plot(range(2,7), silhoutte_scores, "bo-")
plt.xticks([2, 3, 4])
plt.title('Silhoutte scores vs Number of clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Silhoutte score')
plt.show()

"""**Как выбор начальных центров кластеров влияет на результат кластеризации?**

Как известно, k-means позволяет найти не глобальный, а локальный минимум (их может быть несколько). В связи с этим, то, какими были начальные центроиды, влияет на то, в какой из локальных минимумов (то есть допустимых решений задачи) мы придем.

Согласно документации sklearn сейчас по умолчанию в функции KMeans в параметре начальный центроид стоит k-means++. Он считается наиболее эффективным методом поиска начального центроида.

Поделим на 2 кластера
"""

from sklearn.metrics import silhouette_score

kmeans_1 = KMeans(n_clusters=2, random_state=23, init = 'k-means++')
kmeans_1.fit(X_red)

print('Silhoutte score of our model is ' + str(silhouette_score(X_red, kmeans.labels_)))

# посмотрим на координаты начальных центроидов
kmeans_1.cluster_centers_

"""Теперь попробуем с тем же k=2 кластеров изменить начальные центроиды"""

from sklearn.metrics import silhouette_score

kmeans_1_2 = KMeans(n_clusters=2, random_state=23, init = 'random') #заменили k-means++ на random
kmeans_1_2.fit(X_red)

print('Silhoutte score of our model is ' + str(silhouette_score(X_red, kmeans.labels_)))

# посмотрим на координаты начальных центроидов
kmeans_1_2.cluster_centers_

"""Видим, что они изменились, как и метрика качества Силуэт (стала немного хуже).

Тем не менее, разница выглядит несущественной.
"""

#вернемся к k=2 кластеров при инициализации через k-means++. Посмотрим на метрики качества

from sklearn import metrics

Sil = metrics.silhouette_score(X_red, kmeans_1.labels_) #оценка в диапазоне -1 и 1, и более высокий балл указывает на лучшее разделение кластеров
CH = metrics.calinski_harabasz_score(X_red, kmeans_1.labels_) #чем выше значение CH, тем лучше кластеры отделены друг от друга
DB = metrics.davies_bouldin_score(X_red, kmeans_1.labels_) #более низкий показатель сходства указывает на лучшее разделение кластеров
print('Silhoutte score of our model is ' + str(Sil))
print('Kalinski-Harabash Index score of our model is ' + str(CH))
print('Davis-Buldin Index score of our model is ' + str(DB))

#вернемся к k=2 кластеров при инициализации через random. Посмотрим на метрики качества

from sklearn import metrics

Sil = metrics.silhouette_score(X_red, kmeans_1_2.labels_) #оценка в диапазоне -1 и 1, и более высокий балл указывает на лучшее разделение кластеров
CH = metrics.calinski_harabasz_score(X_red, kmeans_1_2.labels_) #чем выше значение CH, тем лучше кластеры отделены друг от друга
DB = metrics.davies_bouldin_score(X_red, kmeans_1_2.labels_) #более низкий показатель сходства указывает на лучшее разделение кластеров
print('Silhoutte score of our model is ' + str(Sil))
print('Kalinski-Harabash Index score of our model is ' + str(CH))
print('Davis-Buldin Index score of our model is ' + str(DB))

"""**Можно заключить, что в данной задаче выбор начальных центроидов незначительно влияет на результат**

Поделим на 3 кластера (кажется более релеватным с учетом того, что задача маркетинговая - чем персонифицированее предложение, тем лучше)
"""

from sklearn.metrics import silhouette_score

kmeans_2 = KMeans(n_clusters=3, random_state=23)
kmeans_2.fit(X_red)

print('Silhoutte score of our model is ' + str(silhouette_score(X_red, kmeans.labels_)))

Sil = metrics.silhouette_score(X_red, kmeans_2.labels_)
CH = metrics.calinski_harabasz_score(X_red, kmeans_2.labels_)
DB = metrics.davies_bouldin_score(X_red, kmeans_2.labels_)
print('Silhoutte score of our model is ' + str(Sil))
print('Kalinski-Harabash Index score of our model is ' + str(CH))
print('Davis-Buldin Index score of our model is ' + str(DB))

"""Видим, что метрика Sil стала менее эффективной, при этом, метрика CH и DB стали лучше.

Поделим на 6 кластеров
"""

from sklearn.metrics import silhouette_score

kmeans_3 = KMeans(n_clusters=6, random_state=23)
kmeans_3.fit(X_red)

print('Silhoutte score of our model is ' + str(silhouette_score(X_red, kmeans_3.labels_)))

Sil = metrics.silhouette_score(X_red, kmeans_3.labels_)
CH = metrics.calinski_harabasz_score(X_red, kmeans_3.labels_)
DB = metrics.davies_bouldin_score(X_red, kmeans_3.labels_)
print('Silhoutte score of our model is ' + str(Sil))
print('Kalinski-Harabash Index score of our model is ' + str(CH))
print('Davis-Buldin Index score of our model is ' + str(DB))

"""Видим, что метрика Sil стала еще менее эффективной, при этом, метрика CH стала лучше, чем два предыдущих варианта.

Сделаем визуальную аналитику качества кластеров

Вспомним, что в матрице корреляции обнаружили, что есть тесная связь между Purchases  - покупками и ONEOFF Purchases - одноразовыми покупками.

Проверим сначала на k=3
"""

#добавим колонку с номером кластеров в изначальный датасет
data['cluster_id'] = kmeans_2.labels_

for col in cols:
    data[col] = np.exp(data[col])

plt.figure(figsize=(10,6))
sns.scatterplot(data=data, x='ONEOFF_PURCHASES', y='PURCHASES', hue='cluster_id')
plt.title('Distribution of clusters based on One off purchases and total purchases')
plt.show()

"""Данная диаграмма рассеивания показывает, что разбиение достаточно хорошее."""

#добавим колонку с номером кластеров в изначальный датасет
data['cluster_id'] = kmeans_3.labels_

for col in cols:
    data[col] = np.exp(data[col])

plt.figure(figsize=(10,6))
sns.scatterplot(data=data, x='ONEOFF_PURCHASES', y='PURCHASES', hue='cluster_id')
plt.title('Distribution of clusters based on One off purchases and total purchases')
plt.show()

"""Можно увидеть, что k=6 тоже достаточно хорошо справился с сегментацией клиентов.

**Отчет**

Данная задача содержит большое количество критериев, связи с этим гипотезы тестировались на выборочных данных.

Мы увидели, что в конкретной задаче способ инициализации центроидов незначительно повлиял на результат - тем не менее, разница есть.

В теории задач кластеризации нет однозначного ответа, какая метрика качества подходит к какому типу задач/размерности задач.
Поскольку в задаче не были даны истинные значения кластеров, мы применяли внутренние метрики. Тем не менее, выбрать какую-то одну нам не удалось. На наш взгляд, это лучше делать итеративно и исходя из специфики поставленной бизнес-задачи.

При выполнениии задания использовались ресурсы:

1. https://scikit-learn.ru/clustering/#k-means
2. https://angelgardt.github.io/hseuxlab-wlm2021/book/cluster.html
3. https://machinelearningknowledge.ai/tutorial-for-k-means-clustering-in-python-sklearn/?ysclid=lqfk177vse526445361#Objective
4. https://skine.ru/articles/8197/?ysclid=lqgqlhffte799571103
"""